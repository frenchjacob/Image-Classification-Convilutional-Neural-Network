{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print('GPU')\n",
    "\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('CPU')\n",
    "\n"
   ],
   "metadata": {
    "id": "NzeMxDP7_9IL",
    "outputId": "314550c5-e53f-4e65-aec1-056613d1e7a9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GPU\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# Define the root directory of your dataset and your desired image size\n",
    "data_root = '/content/drive/MyDrive/traindata'\n",
    "desired_size = (300, 300)  # Adjust to your desired size\n",
    "\n",
    "# Create a dataset with the same transformations you are using\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(desired_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Resize images to the desired size\n",
    "\n",
    "])\n",
    "\n",
    "custom_dataset = ImageFolder(root=data_root, transform=transform)\n",
    "\n"
   ],
   "metadata": {
    "id": "Ud1r5h6zV4Kf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 64\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(custom_dataset))\n",
    "test_size = len(custom_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(custom_dataset, [train_size, test_size])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('cherry','strawberry','tomato')"
   ],
   "metadata": {
    "id": "IUDP2Y22WDLZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class_to_idx = custom_dataset.class_to_idx\n",
    "\n",
    "# Invert the mapping to get class labels to counts\n",
    "class_counts = {class_label: 0 for class_label in class_to_idx}\n",
    "\n",
    "# Count the occurrences of each class label\n",
    "for _, label in train_dataset:\n",
    "    class_counts[custom_dataset.classes[label]] += 1\n",
    "\n",
    "# Print the counts\n",
    "for class_label, count in class_counts.items():\n",
    "    print(f'Class {class_label}: {count} samples')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jXqdP7JY_6fV",
    "outputId": "4288bc5a-307c-4042-89a2-c1278f93f54a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Class cherry: 1183 samples\n",
      "Class strawberry: 1207 samples\n",
      "Class tomato: 1210 samples\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Baseline Model"
   ],
   "metadata": {
    "id": "me3dZ8WgDduS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#split the data into test & training\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3*300*300, 128) #input_size = 3 * 300 * 300\n",
    "        self.fc2 = nn.Linear(128,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Update the view dimension\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ],
   "metadata": {
    "id": "qUYGJsHGSYT-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(5)):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxAkV4l0S_Qk",
    "outputId": "33531b4b-38e9-4466-f05a-ff974f734968"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [06:57<00:00, 83.46s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "net.eval()\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 3000 test images: {100 * correct // total} %')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJU0-PscYTkN",
    "outputId": "14e1d445-7b70-4f32-b53e-0fe541254acc"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the 3000 test images: 51 %\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Basic Convilutional Network"
   ],
   "metadata": {
    "id": "ZSCUdn8vDjXu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 72 * 72, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ],
   "metadata": {
    "id": "DWrAW7hSFeHX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(5)):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9WAS4rzFk1R",
    "outputId": "7b2f9739-8aeb-4232-d907-6440a5df4034"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [01:03<00:00, 12.77s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "net.eval()\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 3000 test images: {100 * correct // total} %')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etXN047sHAxv",
    "outputId": "ee604a58-d557-4ef9-e719-e3ed9a279b62"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the 3000 test images: 55 %\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cross-Validating Optimisers"
   ],
   "metadata": {
    "id": "P-8ln8BKJYwR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "folds = 3\n",
    "classes = ('cherry', 'strawberry', 'tomato')\n",
    "\n",
    "# Pre-split your dataset into a training set and a test set\n",
    "train_size = int(0.8 * len(custom_dataset))\n",
    "test_size = len(custom_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(custom_dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create a KFold object to handle the splits for the training set\n",
    "kf = KFold(n_splits=folds, shuffle=True)\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(train_dataset), 1):\n",
    "    print(f\"Fold {fold}:\")\n",
    "\n",
    "    # Create a validation dataset for this fold\n",
    "    train_split_dataset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(train_dataset, val_indices)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_split_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    net = Net()\n",
    "    net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    for epoch in tqdm(range(5)):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the validation set: {accuracy:.2f}%')\n",
    "\n",
    "    # You can save the model with the best validation performance here (if needed)\n",
    "\n",
    "# Test the model on the separate test set\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the network on the test set: {accuracy:.2f}%')\n",
    "\n",
    "print(\"K-fold cross-validation and testing on the test set completed.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VeMSPt5HQquy",
    "outputId": "dafc299c-3e49-4d2b-8c36-bccb6e6a2bd0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fold 1:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:55<00:00, 11.00s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the validation set: 52.00%\n",
      "Fold 2:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:50<00:00, 10.18s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the validation set: 48.50%\n",
      "Fold 3:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:53<00:00, 10.63s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the validation set: 43.33%\n",
      "Accuracy of the network on the test set: 42.78%\n",
      "K-fold cross-validation and testing on the test set completed.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adam"
   ],
   "metadata": {
    "id": "Lmkb4rbQSyq5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "folds = 3\n",
    "classes = ('cherry', 'strawberry', 'tomato')\n",
    "\n",
    "# Pre-split your dataset into a training set and a test set\n",
    "train_size = int(0.8 * len(custom_dataset))\n",
    "test_size = len(custom_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(custom_dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create a KFold object to handle the splits for the training set\n",
    "kf = KFold(n_splits=folds, shuffle=True)\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(train_dataset), 1):\n",
    "    print(f\"Fold {fold}:\")\n",
    "\n",
    "    # Create a validation dataset for this fold\n",
    "    train_split_dataset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(train_dataset, val_indices)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_split_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    net = Net()\n",
    "    net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)  # Use the Adam optimizer\n",
    "\n",
    "    for epoch in tqdm(range(5)):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the validation set: {accuracy:.2f}%')\n",
    "\n",
    "    # You can save the model with the best validation performance here (if needed)\n",
    "\n",
    "# Test the model on the separate test set\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the network on the test set: {accuracy:.2f}%')\n",
    "\n",
    "print(\"K-fold cross-validation and testing on the test set completed.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nd6YCC7vSlX5",
    "outputId": "ef749510-ad98-478c-caee-8e0f480f1613"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fold 1:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:46<00:00,  9.30s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the validation set: 54.58%\n",
      "Fold 2:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:43<00:00,  8.77s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the validation set: 52.42%\n",
      "Fold 3:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:44<00:00,  8.91s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the validation set: 50.00%\n",
      "Accuracy of the network on the test set: 50.22%\n",
      "K-fold cross-validation and testing on the test set completed.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "(54.58+52.42+50)/3"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m_Likh4PVhtJ",
    "outputId": "794b3dc7-e182-4e94-f987-9821a4b05791"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "52.333333333333336"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "RMSProp"
   ],
   "metadata": {
    "id": "BH4zAp0eS8hk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "folds = 3\n",
    "classes = ('cherry', 'strawberry', 'tomato')\n",
    "\n",
    "# Pre-split your dataset into a training set and a test set\n",
    "train_size = int(0.8 * len(custom_dataset))\n",
    "test_size = len(custom_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(custom_dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create a KFold object to handle the splits for the training set\n",
    "kf = KFold(n_splits=folds, shuffle=True)\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(train_dataset), 1):\n",
    "    print(f\"Fold {fold}:\")\n",
    "\n",
    "    # Create a validation dataset for this fold\n",
    "    train_split_dataset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(train_dataset, val_indices)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_split_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    net = Net()\n",
    "    net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=0.001)  # Use the RMSprop optimizer\n",
    "\n",
    "    for epoch in tqdm(range(5)):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network on the validation set: {accuracy:.2f}%')\n",
    "\n",
    "    # You can save the model with the best validation performance here (if needed)\n",
    "\n",
    "# Test the model on the separate test set\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the network on the test set: {accuracy:.2f}%')\n",
    "\n",
    "print(\"K-fold cross-validation and testing on the test set completed.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PA7SzsdS-XD",
    "outputId": "334d779a-07d7-4479-cf04-90adfe07277a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fold 1:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:47<00:00,  9.57s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the validation set: 41.17%\n",
      "Fold 2:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:45<00:00,  9.05s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the validation set: 48.75%\n",
      "Fold 3:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:43<00:00,  8.70s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the validation set: 46.83%\n",
      "Accuracy of the network on the test set: 48.00%\n",
      "K-fold cross-validation and testing on the test set completed.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "net = Net()\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)  # Use the Adam optimizer\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(5)):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zvUnxIuQWNbN",
    "outputId": "4165fd55-97c0-4c82-bd79-fad22c7113c0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:55<00:00, 11.04s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "net.eval()\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 3000 test images: {100 * correct // total} %')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cIijxoagWLz",
    "outputId": "869e0eda-0dee-440e-e02e-8bd7c7c9e6e3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the 3000 test images: 62 %\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AtmLoQ-ug4Ab",
    "outputId": "f031ee07-53aa-4b48-c11a-4d9b4316f24a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy for class: cherry is 50.3 %\n",
      "Accuracy for class: strawberry is 49.7 %\n",
      "Accuracy for class: tomato is 88.8 %\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "MiniBatches"
   ],
   "metadata": {
    "id": "XxdEdagxW9sL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "results = pd.DataFrame(columns=[\"Batch Size\", \"Fold 1\", \"Fold 2\", \"Fold 3\", \"Average\", \"Time\"])\n",
    "\n",
    "# Define the list of batch sizes to test\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "folds = 3\n",
    "classes = ('cherry', 'strawberry', 'tomato')\n",
    "\n",
    "# Create a KFold object to handle the splits for the training set\n",
    "kf = KFold(n_splits=folds, shuffle=True)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    batch_scores = []\n",
    "    batch_time = []\n",
    "    for fold, (train_indices, val_indices) in enumerate(kf.split(train_dataset), 1):\n",
    "        print(f\"Testing Batch Size {batch_size}, Fold {fold}:\")\n",
    "\n",
    "        train_split_dataset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "        val_dataset = torch.utils.data.Subset(train_dataset, val_indices)\n",
    "\n",
    "        trainloader = torch.utils.data.DataLoader(train_split_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "        net = Net()\n",
    "        net.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in tqdm(range(5)):\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Test the model on the separate test set\n",
    "        net.eval()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in valloader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy_val = 100 * correct / total\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        # Append fold score to batch_scores\n",
    "        batch_scores.append(accuracy_val)\n",
    "        # Append elapsed time to batch_time\n",
    "        batch_time.append(elapsed_time)\n",
    "\n",
    "    # Calculate the average score for the current batch size\n",
    "    average_score = sum(batch_scores) / len(batch_scores)\n",
    "\n",
    "    # Add the results to the DataFrame\n",
    "    results = results.append({\n",
    "        \"Batch Size\": batch_size,\n",
    "        \"Fold 1\": f\"{batch_scores[0]:.2f}%\",\n",
    "        \"Fold 2\": f\"{batch_scores[1]:.2f}%\",\n",
    "        \"Fold 3\": f\"{batch_scores[2]:.2f}%\",\n",
    "        \"Average\": f\"{average_score:.2f}%\",\n",
    "        \"Time\": sum(batch_time)\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results)\n",
    "\n",
    "# Calculate and display the average scores for each batch size\n",
    "average_results = results.groupby(\"Batch Size\").mean()\n",
    "average_results = average_results.drop(columns=\"Time\")\n",
    "print(\"\\nAverage Results:\")\n",
    "print(average_results)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "peKgnaveb1tK",
    "outputId": "72355af6-bb6b-4c2f-a38a-b4cf49a40375"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Batch Size 32, Fold 1:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:51<00:00, 10.39s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Batch Size 32, Fold 2:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:44<00:00,  8.92s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Batch Size 32, Fold 3:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:46<00:00,  9.34s/it]\n",
      "<ipython-input-27-c03d4b7cd8eb>:81: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Batch Size 64, Fold 1:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:44<00:00,  8.90s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Batch Size 64, Fold 2:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:44<00:00,  8.83s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Batch Size 64, Fold 3:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:45<00:00,  9.05s/it]\n",
      "<ipython-input-27-c03d4b7cd8eb>:81: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Batch Size 128, Fold 1:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:45<00:00,  9.16s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Batch Size 128, Fold 2:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:44<00:00,  8.96s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Batch Size 128, Fold 3:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:44<00:00,  8.92s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Batch Size  Fold 1  Fold 2  Fold 3 Average        Time\n",
      "0         32  51.58%  53.75%  46.83%  50.72%  156.538980\n",
      "1         64  53.25%  49.75%  55.75%  52.92%  149.654063\n",
      "2        128  52.42%  41.33%  49.58%  47.78%  149.377690\n",
      "\n",
      "Average Results:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [32, 64, 128]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-27-c03d4b7cd8eb>:81: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n",
      "<ipython-input-27-c03d4b7cd8eb>:94: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  average_results = results.groupby(\"Batch Size\").mean()\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Learning Rate"
   ],
   "metadata": {
    "id": "oJb7npcDij5s"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "results = pd.DataFrame(columns=[\"Learning Rate\", \"Fold 1\", \"Fold 2\", \"Fold 3\", \"Average\", \"Time\"])\n",
    "\n",
    "# Define the list of learning rates to test\n",
    "learning_rates = [0.001, 0.0015, 0.003, 0.005, 0.01]\n",
    "\n",
    "batch_size = 64\n",
    "folds = 3\n",
    "classes = ('cherry', 'strawberry', 'tomato')\n",
    "\n",
    "# Create a KFold object to handle the splits for the training set\n",
    "kf = KFold(n_splits=folds, shuffle=True)\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    batch_scores = []\n",
    "    batch_time = []\n",
    "    for fold, (train_indices, val_indices) in enumerate(kf.split(train_dataset), 1):\n",
    "        print(f\"Testing Learning Rate {learning_rate}, Batch Size {batch_size}, Fold {fold}:\")\n",
    "\n",
    "        train_split_dataset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "        val_dataset = torch.utils.data.Subset(train_dataset, val_indices)\n",
    "\n",
    "        trainloader = torch.utils.data.DataLoader(train_split_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "        net = Net()\n",
    "        net.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in tqdm(range(5)):\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Test the model on the separate test set\n",
    "        net.eval()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in valloader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy_val = 100 * correct / total\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        # Append fold score to batch_scores\n",
    "        batch_scores.append(accuracy_val)\n",
    "        # Append elapsed time to batch_time\n",
    "        batch_time.append(elapsed_time)\n",
    "\n",
    "    # Calculate the average score for the current learning rate\n",
    "    average_score = sum(batch_scores) / len(batch_scores)\n",
    "\n",
    "    # Add the results to the DataFrame\n",
    "    results = results.append({\n",
    "        \"Learning Rate\": learning_rate,\n",
    "        \"Fold 1\": f\"{batch_scores[0]:.2f}%\",\n",
    "        \"Fold 2\": f\"{batch_scores[1]:.2f}%\",\n",
    "        \"Fold 3\": f\"{batch_scores[2]:.2f}%\",\n",
    "        \"Average\": f\"{average_score:.2f}%\",\n",
    "        \"Time\": sum(batch_time)\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPiw5topilU4",
    "outputId": "9297b1fc-9c81-41b1-ff7c-42b59c96ab2c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Learning Rate 0.001, Batch Size 64, Fold 1:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:48<00:00,  9.74s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Learning Rate 0.001, Batch Size 64, Fold 2:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:45<00:00,  9.16s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Learning Rate 0.001, Batch Size 64, Fold 3:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:45<00:00,  9.04s/it]\n",
      "<ipython-input-34-db3c35a62754>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Learning Rate 0.0015, Batch Size 64, Fold 1:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:45<00:00,  9.08s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Learning Rate 0.0015, Batch Size 64, Fold 2:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:44<00:00,  8.85s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Learning Rate 0.0015, Batch Size 64, Fold 3:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:45<00:00,  9.13s/it]\n",
      "<ipython-input-34-db3c35a62754>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Learning Rate 0.003, Batch Size 64, Fold 1:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:46<00:00,  9.27s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Learning Rate 0.003, Batch Size 64, Fold 2:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:44<00:00,  8.90s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Learning Rate 0.003, Batch Size 64, Fold 3:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:44<00:00,  8.99s/it]\n",
      "<ipython-input-34-db3c35a62754>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Learning Rate 0.005, Batch Size 64, Fold 1:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:46<00:00,  9.29s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Learning Rate 0.005, Batch Size 64, Fold 2:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:46<00:00,  9.29s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Learning Rate 0.005, Batch Size 64, Fold 3:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:46<00:00,  9.23s/it]\n",
      "<ipython-input-34-db3c35a62754>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Learning Rate 0.01, Batch Size 64, Fold 1:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:44<00:00,  8.86s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Learning Rate 0.01, Batch Size 64, Fold 2:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:44<00:00,  8.88s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Learning Rate 0.01, Batch Size 64, Fold 3:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:45<00:00,  9.17s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Learning Rate  Fold 1  Fold 2  Fold 3 Average        Time\n",
      "0         0.0010  55.50%  50.08%  54.75%  53.44%  152.383169\n",
      "1         0.0015  52.00%  49.17%  52.42%  51.19%  149.701398\n",
      "2         0.0030  46.08%  42.25%  33.92%  40.75%  149.672995\n",
      "3         0.0050  33.92%  31.92%  33.33%  33.06%  152.402562\n",
      "4         0.0100  33.92%  31.83%  32.58%  32.78%  150.866724\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-34-db3c35a62754>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Number of Convolutional Layers"
   ],
   "metadata": {
    "id": "_oihmQrJn13F"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Conv1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(131424, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 72 * 72, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Conv3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 5)\n",
    "        self.fc1 = nn.Linear(36992, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Conv4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 5)\n",
    "        self.conv4 = nn.Conv2d(32, 64, 5)\n",
    "        self.fc1 = nn.Linear(14400, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Conv5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 5)\n",
    "        self.conv4 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv5 = nn.Conv2d(64, 128, 5)\n",
    "        self.fc1 = nn.Linear(3200, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.pool(F.relu(self.conv5(x)))\n",
    "        x = x.view(x.size(0), -1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "id": "AAnBTWbsn4eg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "\n",
    "# To ignore all warnings (not recommended unless you know the risks)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "results = pd.DataFrame(columns=[\"Model\", \"Fold 1\", \"Fold 2\", \"Fold 3\", \"Average\", \"Time\"])\n",
    "\n",
    "# Define the list of models to test\n",
    "models = [Conv5]\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "folds = 3\n",
    "classes = ('cherry', 'strawberry', 'tomato')\n",
    "\n",
    "# Create a KFold object to handle the splits for the training set\n",
    "kf = KFold(n_splits=folds, shuffle=True)\n",
    "\n",
    "for model_class in models:\n",
    "    batch_scores = []\n",
    "    batch_time = []\n",
    "    for fold, (train_indices, val_indices) in enumerate(kf.split(train_dataset), 1):\n",
    "        print(f\"Testing Model {model_class.__name__}, Fold {fold}:\")\n",
    "\n",
    "        train_split_dataset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "        val_dataset = torch.utils.data.Subset(train_dataset, val_indices)\n",
    "\n",
    "        trainloader = torch.utils.data.DataLoader(train_split_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "        net = model_class()\n",
    "        net.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in tqdm(range(5)):\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Test the model on the separate test set\n",
    "        net.eval()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in valloader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy_val = 100 * correct / total\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        # Append fold score to batch_scores\n",
    "        batch_scores.append(accuracy_val)\n",
    "        # Append elapsed time to batch_time\n",
    "        batch_time.append(elapsed_time)\n",
    "\n",
    "    # Calculate the average score for the current model\n",
    "    average_score = sum(batch_scores) / len(batch_scores)\n",
    "\n",
    "    # Add the results to the DataFrame\n",
    "    results = results.append({\n",
    "        \"Model\": model_class.__name__,\n",
    "        \"Fold 1\": f\"{batch_scores[0]:.2f}%\",\n",
    "        \"Fold 2\": f\"{batch_scores[1]:.2f}%\",\n",
    "        \"Fold 3\": f\"{batch_scores[2]:.2f}%\",\n",
    "        \"Average\": f\"{average_score:.2f}%\",\n",
    "        \"Time\": sum(batch_time)\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TSNgfQ7joZjk",
    "outputId": "7dda74d8-ed8a-43cd-cc74-2146ecf47917"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Model Conv5, Fold 1:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:47<00:00,  9.42s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Model Conv5, Fold 2:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:46<00:00,  9.25s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Model Conv5, Fold 3:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:45<00:00,  9.10s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Model  Fold 1  Fold 2  Fold 3 Average        Time\n",
      "0  Conv5  54.00%  48.58%  54.17%  52.25%  154.642134\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing on Entire Test Set"
   ],
   "metadata": {
    "id": "9vJoykgfsJKY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "net = Conv4()\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)  # Use the Adam optimizer\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(5)):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GgxFVTI0sIXn",
    "outputId": "d49dd2fa-1c13-4353-ed8c-6699e6f54a2a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:48<00:00,  9.61s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "net.eval()\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 3000 test images: {100 * correct // total} %')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qrpTitoVsWJI",
    "outputId": "847e9a8e-c0d1-423f-bf9a-00abcf6e6452"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the 3000 test images: 60 %\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2ZQl0kMsdVB",
    "outputId": "7cfd9bca-1725-42c2-d0da-70e2a6780239"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy for class: cherry is 52.3 %\n",
      "Accuracy for class: strawberry is 63.8 %\n",
      "Accuracy for class: tomato is 67.1 %\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tune Epochs"
   ],
   "metadata": {
    "id": "zUkmWY_dwf2R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "\n",
    "# To ignore all warnings (not recommended unless you know the risks)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "results = pd.DataFrame(columns=[\"Model\", \"Epochs\", \"Fold 1\", \"Fold 2\", \"Fold 3\", \"Average\", \"Time\"])\n",
    "\n",
    "# Define the list of epochs to test\n",
    "epochs_list = [5, 10, 15]\n",
    "batch_size = 64\n",
    "folds = 3\n",
    "learning_rate = 0.001\n",
    "classes = ('cherry', 'strawberry', 'tomato')\n",
    "\n",
    "# Create a KFold object to handle the splits for the training set\n",
    "kf = KFold(n_splits=folds, shuffle=True)\n",
    "\n",
    "for epochs in epochs_list:\n",
    "    batch_scores = []\n",
    "    batch_time = []\n",
    "    for fold, (train_indices, val_indices) in enumerate(kf.split(train_dataset), 1):\n",
    "        print(f\"Testing Model Conv4, Epochs {epochs}, Fold {fold}:\")\n",
    "\n",
    "        train_split_dataset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "        val_dataset = torch.utils.data.Subset(train_dataset, val_indices)\n",
    "\n",
    "        trainloader = torch.utils.data.DataLoader(train_split_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "        net = Conv4()\n",
    "        net.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Test the model on the separate test set\n",
    "        net.eval()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in valloader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy_val = 100 * correct / total\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        # Append fold score to batch_scores\n",
    "        batch_scores.append(accuracy_val)\n",
    "        # Append elapsed time to batch_time\n",
    "        batch_time.append(elapsed_time)\n",
    "\n",
    "    # Calculate the average score for the current number of epochs\n",
    "    average_score = sum(batch_scores) / len(batch_scores)\n",
    "\n",
    "    # Add the results to the DataFrame\n",
    "    results = results.append({\n",
    "        \"Model\": \"Conv4\",\n",
    "        \"Epochs\": epochs,\n",
    "        \"Fold 1\": f\"{batch_scores[0]:.2f}%\",\n",
    "        \"Fold 2\": f\"{batch_scores[1]:.2f}%\",\n",
    "        \"Fold 3\": f\"{batch_scores[2]:.2f}%\",\n",
    "        \"Average\": f\"{average_score:.2f}%\",\n",
    "        \"Time\": sum(batch_time)\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Ivs2EXswhvG",
    "outputId": "aa22d212-2191-4505-aaa1-3b54d002f393"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Model Conv4, Epochs 5, Fold 1:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:54<00:00, 10.90s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Model Conv4, Epochs 5, Fold 2:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:49<00:00,  9.95s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Model Conv4, Epochs 5, Fold 3:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:45<00:00,  9.10s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Model Conv4, Epochs 10, Fold 1:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [01:35<00:00,  9.57s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Model Conv4, Epochs 10, Fold 2:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [01:34<00:00,  9.49s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Model Conv4, Epochs 10, Fold 3:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [01:35<00:00,  9.51s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Model Conv4, Epochs 15, Fold 1:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 15/15 [02:22<00:00,  9.47s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Model Conv4, Epochs 15, Fold 2:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 15/15 [02:24<00:00,  9.63s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Model Conv4, Epochs 15, Fold 3:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 15/15 [02:22<00:00,  9.52s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Model Epochs  Fold 1  Fold 2  Fold 3 Average        Time\n",
      "0  Conv4      5  63.67%  58.50%  61.08%  61.08%  164.105112\n",
      "1  Conv4     10  71.00%  63.75%  65.17%  66.64%  299.549843\n",
      "2  Conv4     15  56.50%  67.33%  62.58%  62.14%  445.004826\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing on entire training set"
   ],
   "metadata": {
    "id": "-SYi50Mt094j"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "net = Conv4()\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)  # Use the Adam optimizer\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(10)):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZAEx4N_5081n",
    "outputId": "f8d5fc6c-6ae5-4303-ee4e-15832df8224f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [01:59<00:00, 11.93s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "net.eval()\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 3000 test images: {100 * correct // total} %')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PucmX9XP1FJq",
    "outputId": "c09ef2f6-4f9d-40f4-9182-0158ad9bebb1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the 3000 test images: 58 %\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oihG1NWc1IRo",
    "outputId": "7978d801-a6e3-4512-ad48-54bbd060b368"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy for class: cherry is 40.0 %\n",
      "Accuracy for class: strawberry is 86.5 %\n",
      "Accuracy for class: tomato is 49.7 %\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "j4VnHvdz1Jsg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Augmenting the Dataset with Horizontal Flip"
   ],
   "metadata": {
    "id": "w6PSTKqK1Ksl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Define your original dataset directory and destination directories\n",
    "original_dataset_dir = '/content/drive/MyDrive/traindata'\n",
    "train_dir = '/content/drive/MyDrive/train'\n",
    "test_dir = '/content/drive/MyDrive/test'\n",
    "\n",
    "\n",
    "# Define the split ratio (e.g., 80% for training, 20% for testing)\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Create destination directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through the subdirectories in your original dataset\n",
    "for class_dir in os.listdir(original_dataset_dir):\n",
    "    class_path = os.path.join(original_dataset_dir, class_dir)\n",
    "\n",
    "    # Create subdirectories in the train and test directories\n",
    "    train_class_dir = os.path.join(train_dir, class_dir)\n",
    "    test_class_dir = os.path.join(test_dir, class_dir)\n",
    "    os.makedirs(train_class_dir, exist_ok=True)\n",
    "    os.makedirs(test_class_dir, exist_ok=True)\n",
    "\n",
    "    # List all the files in the class directory\n",
    "    files = os.listdir(class_path)\n",
    "\n",
    "    # Randomly shuffle the list of files\n",
    "    random.shuffle(files)\n",
    "\n",
    "    # Determine the split point based on the split_ratio\n",
    "    split_point = int(len(files) * split_ratio)\n",
    "\n",
    "    # Copy or move files to the train and test directories\n",
    "    for i, file in enumerate(files):\n",
    "        src = os.path.join(class_path, file)\n",
    "        if i < split_point:\n",
    "            dst = os.path.join(train_class_dir, file)\n",
    "        else:\n",
    "            dst = os.path.join(test_class_dir, file)\n",
    "        shutil.copy(src, dst)  # Use shutil.move if you want to move the files\n",
    "\n",
    "print(\"Data split and stored into training and test directories.\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SyQkSA33IZw",
    "outputId": "8911ced3-70f7-480d-cf7a-22949810ff65"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data split and stored into training and test directories.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# Define the root directory of your dataset and your desired image size\n",
    "train_root = '/content/drive/MyDrive/train'\n",
    "test_root = '/content/drive/MyDrive/test'\n",
    "desired_size = (300, 300)  # Adjust to your desired size\n",
    "\n",
    "# Create a dataset with the same transformations you are using\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(desired_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5)# Resize images to the desired size\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(desired_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))# Resize images to the desired size\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(root=train_root, transform=train_transform)\n",
    "test_dataset = ImageFolder(root=test_root, transform=test_transform)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('cherry','strawberry','tomato')"
   ],
   "metadata": {
    "id": "tDvPxUxl-FVj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "net = Conv4()\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)  # Use the Adam optimizer\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(27)):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "net.eval()\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 3000 test images: {100 * correct // total} %')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fMTHNwTbBj9E",
    "outputId": "61588667-abf7-43e1-ce7b-dd13519a639b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 27/27 [05:58<00:00, 13.26s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the 3000 test images: 77 %\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "net.eval()\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 3000 test images: {100 * correct // total} %')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zu72FvWoBrlX",
    "outputId": "59f75455-4118-408d-9ae1-b12f04835c4f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the 3000 test images: 77 %\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33EL6YXWCEC5",
    "outputId": "13d209a8-e99b-46bb-de82-28f3afe24d2e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy for class: cherry is 80.0 %\n",
      "Accuracy for class: strawberry is 79.7 %\n",
      "Accuracy for class: tomato is 72.7 %\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Random Rotation"
   ],
   "metadata": {
    "id": "gWeMRVWWDt5u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# Define the root directory of your dataset and your desired image size\n",
    "train_root = '/content/drive/MyDrive/train'\n",
    "test_root = '/content/drive/MyDrive/test'\n",
    "desired_size = (300, 300)  # Adjust to your desired size\n",
    "\n",
    "# Create a dataset with the same transformations you are using\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(desired_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees = (-30,30))# Resize images to the desired size\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(desired_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))# Resize images to the desired size\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(root=train_root, transform=train_transform)\n",
    "test_dataset = ImageFolder(root=test_root, transform=test_transform)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('cherry','strawberry','tomato')"
   ],
   "metadata": {
    "id": "5kglCqSPDwvW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "net = Conv4()\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)  # Use the Adam optimizer\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(10)):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "net.eval()\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 3000 test images: {100 * correct // total} %')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJtYhh-eD2oY",
    "outputId": "f47f42a9-ca3b-43e4-c276-3b8f2320ed32"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [05:01<00:00, 30.19s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the 3000 test images: 63 %\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transfer Learning"
   ],
   "metadata": {
    "id": "25hVlZH3L1JT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torchvision.models import resnet18\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "model = resnet18(weights='imagenet')\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "model.fc = nn.Linear(num_ftrs,3)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum = 0.9)\n",
    "\n",
    "#step_lr_scheduler = lr_scheduler.StepLR(optimizer,step_size=7, gamma=0.1)\n",
    "\n",
    "for epoch in tqdm(range(20)):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(inputs)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 3000 test images: {100 * correct // total} %')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCbRsJVDEDfJ",
    "outputId": "d12afff2-bd8a-4850-8b90-88bf6fcd83a8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [05:37<00:00, 16.89s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the 3000 test images: 91 %\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mBpethLiLt9Z",
    "outputId": "94e60d98-994d-4730-e730-657a07392f8d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy for class: cherry is 91.0 %\n",
      "Accuracy for class: strawberry is 96.3 %\n",
      "Accuracy for class: tomato is 89.0 %\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def train_show(network, data, targ, lossFunc, optimiser, epochs):\n",
    "    lossHistory = []  # just to show a plot later...\n",
    "    accuHistory = []\n",
    "\n",
    "    for t in range(epochs):\n",
    "        optimiser.zero_grad()      # Gradients accumulate by default, so don't forget to do this.\n",
    "\n",
    "        y = network.forward(data)  # the forward pass\n",
    "\n",
    "        loss = lossFunc(y,targ)    # recompute the loss\n",
    "        loss.backward()            # runs autograd, to get the gradients needed by optimiser\n",
    "        optimiser.step()           # take a step\n",
    "\n",
    "        # just housekeeping and reporting\n",
    "        accuracy = torch.mean((torch.argmax(y,dim=1) == targ).float())\n",
    "        lossHistory.append(loss.detach().item())\n",
    "        accuHistory.append(accuracy.detach())\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(lossHistory,'r'); plt.title(\"loss\"); plt.xlabel(\"epochs\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(accuHistory,'b'); plt.title(\"accuracy\")"
   ],
   "metadata": {
    "id": "pmbsmZBvxjoa"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
